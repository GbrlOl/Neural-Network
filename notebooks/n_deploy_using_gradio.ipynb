{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "transform_to_gray = transforms.Compose([\n",
    "    transforms.Resize((200,200)),\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convertir a escala de grises\n",
    "    transforms.ConvertImageDtype(dtype=torch.float32)  # Convertir a flotante si es necesario\n",
    "])\n",
    "\n",
    "image = transform_to_gray(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "holi = image.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holi.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "otra = holi.squeeze()\n",
    "\n",
    "plt.imshow(otra, cmap='gray')\n",
    "plt.savefig(\"test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "tensor = torch.from_numpy(lista[0]['composite'][:,:,3]).unsqueeze(0)\n",
    "# tensor = torch.\n",
    "# plt.imshow(tensor, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[255, 255, 255,  ...,   0,   0,   0],\n",
       "         [255, 255, 255,  ...,   0,   0,   0],\n",
       "         [255, 255, 255,  ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[255, 255, 255,  ...,   0,   0,   0],\n",
       "         [255, 255, 255,  ...,   0,   0,   0],\n",
       "         [255, 255, 255,  ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "modificar_size = Resize((200,200))\n",
    "tensor_modificado = modificar_size(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[255, 255, 255,  ...,   0,   0,   0],\n",
       "         [255, 255, 255,  ...,   0,   0,   0],\n",
       "         [255, 255, 255,  ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_modificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.uint8"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CNN(nn.Module):\n",
    "  def __init__(self, in_channels=1, num_classes=4):\n",
    "    super(CNN, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "    self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "    self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "    # self.fc1 = nn.Linear(16*7*7, num_classes)\n",
    "    self.fc1 = nn.Linear(200*200, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.conv1(x))\n",
    "    x = self.pool(x)\n",
    "    x = F.relu(self.conv2(x))\n",
    "    x = self.pool(x)\n",
    "    x = x.reshape(x.shape[0], -1)\n",
    "    x = self.fc1(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN()\n",
    "model.load_state_dict(torch.load('gabriel_modelo.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, imagen, device=\"cpu\"):\n",
    "    label_mapping = {0: \"círculo\", 1: \"triángulo\", 2: \"cuadrado\", 3: \"estrella\"}\n",
    "\n",
    "    model.eval()  # Ponemos el modelo en modo evaluación\n",
    "\n",
    "    # Realizar la inferencia\n",
    "    with torch.no_grad():\n",
    "        scores = model(imagen)\n",
    "        _, prediction = scores.max(1)\n",
    "        label_predicho = prediction.item()\n",
    "\n",
    "    # Obtener nombres de etiquetas\n",
    "    # label_real_name = label_mapping.get(label_real, \"Desconocido\")\n",
    "    label_predicho_name = label_mapping.get(label_predicho, \"Desconocido\")\n",
    "\n",
    "    # model.train()  # Restauramos el modelo a modo entrenamiento\n",
    "    \n",
    "    return label_predicho_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict(img):  \n",
    "    # 1º: Recibimos un diccionario con 3 keys(), dentro contienen distintos arrays, accedemos al array de nuestro interés\n",
    "    # Cabe indicar que el tamaño del array es de (800,800)\n",
    "    image_array = img['composite'][:,:,3]\n",
    "    image_array = 255 - image_array\n",
    "\n",
    "    # listita.append(image_array)\n",
    "    # 2º: Convertirmos el array a un tensor, el cual tendrá un ndim=2 y dtype=uint8. Debemos agregar dos dimensiones y convertir de uint8 a float32\n",
    "    #     Utilizando unsqueeze(0) agregamos una dimensión, para que quede de la siguiente forma (1,800,800)\n",
    "    image_tensor = torch.from_numpy(image_array).unsqueeze(0)\n",
    "\n",
    "    # 3º: Debemos seguir modificando las dimensiones para que queden de la forma en que se entrenó el modelo \n",
    "    # a) El modelo recibe un tensor con ndim=4, dtype=torch.float32 y un shape=(64, 1, 200, 200)\n",
    "    transform_to_gray = transforms.Compose([\n",
    "    transforms.Resize((200,200)),\n",
    "    # transforms.Grayscale(num_output_channels=1),  # Convertir a escala de grises\n",
    "    transforms.ConvertImageDtype(dtype=torch.float32)  # Convertir a flotante si es necesario\n",
    "    ])\n",
    "\n",
    "    image = transform_to_gray(image_tensor)\n",
    "\n",
    "    # 4º: Agregamos la útlima dimensión:\n",
    "    image = image.unsqueeze(0)\n",
    "\n",
    "    # 5º: Hacemos la inferencia\n",
    "    label_predict = inference(model, image, device=\"cpu\")\n",
    "    print(label_predict)\n",
    "\n",
    "    # 6º: Convertimos la imagen devuelta\n",
    "    img_devuelta = image.squeeze()\n",
    "    plt.imshow(img_devuelta, cmap=\"gray\")\n",
    "    plt.title(f\"Predicción: {label_predict}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"Predicción {label_predict}.png\")\n",
    "\n",
    "    return label_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gol_m\\anaconda3\\envs\\programacion_2\\lib\\site-packages\\gradio\\components\\image_editor.py:253: UserWarning: `crop_size` parameter is deprecated. Please use `canvas_size` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triángulo\n",
      "triángulo\n",
      "triángulo\n",
      "triángulo\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        im = gr.Sketchpad(\n",
    "            type=\"numpy\",\n",
    "            crop_size=\"1:1\",\n",
    "            # image_mode='L',\n",
    "            # layers=True\n",
    "        )\n",
    "        im_preview = gr.Image()\n",
    "\n",
    "        out = gr.Label()\n",
    "\n",
    "    im.change(predict, outputs=im_preview, inputs=im, show_progress=\"hidden\")\n",
    "\n",
    "demo.launch(share=False, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1º: Recibimos un diccionario con 3 keys(), dentro contienen distintos arrays, accedemos al array de nuestro interés\n",
    "# Cabe indicar que el tamaño del array es de (800,800)\n",
    "image_array = img['composite'][:,:,3]\n",
    "listita.append(image_array)\n",
    "# 2º: Convertirmos el array a un tensor, el cual tendrá un ndim=2 y dtype=uint8. Debemos agregar dos dimensiones y convertir de uint8 a float32\n",
    "#     Utilizando unsqueeze(0) agregamos una dimensión, para que quede de la siguiente forma (1,800,800)\n",
    "image_tensor = torch.from_numpy(image_array).unsqueeze(0)\n",
    "\n",
    "# 3º: Debemos seguir modificando las dimensiones para que queden de la forma en que se entrenó el modelo \n",
    "# a) El modelo recibe un tensor con ndim=4, dtype=torch.float32 y un shape=(64, 1, 200, 200)\n",
    "transform_to_gray = transforms.Compose([\n",
    "transforms.Resize((200,200)),\n",
    "# transforms.Grayscale(num_output_channels=1),  # Convertir a escala de grises\n",
    "transforms.ConvertImageDtype(dtype=torch.float32)  # Convertir a flotante si es necesario\n",
    "])\n",
    "\n",
    "image = transform_to_gray(image_tensor)\n",
    "\n",
    "# 4º: Agregamos la útlima dimensión:\n",
    "image = image.unsqueeze(0)\n",
    "\n",
    "# 5º: Hacemos la inferencia\n",
    "label_predict = inference(model, image, device=\"cpu\")\n",
    "print(label_predict)\n",
    "\n",
    "img_devuelta = image.squeeze()\n",
    "plt.imshow(img_devuelta, cmap=\"gray\")\n",
    "plt.title(f\"Predicción: {label_predict}\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "programacion_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
